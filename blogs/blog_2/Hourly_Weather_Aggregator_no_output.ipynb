{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing Free Hourly Weather Data\n",
    "NOAA stores Free Hourly Weather Data at: ftp://ftp.ncdc.noaa.gov/pub/data/noaa/\n",
    "\n",
    "It is my understanding that this is the data underlying most of the websites that charge for historical weather data at the hourly level (at least when needed in bulk).\n",
    "\n",
    "The most detailed data is in a very cumbersome format, but a subset of easy to parse data can be found at: ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-lite/\n",
    "\n",
    "To make this topic approachable to a large audience, I've added some notes on how one could access this data simply using Excel at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Names were determined from ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-lite/isd-lite-format.pdf\n",
    "# That pdf describes what data is contained in the subset of data that I'll focus on.\n",
    "isd_fwf_cols = ['year', 'month', 'day', 'hour', 'air_temp_c', 'dew_pt_temp_c',\n",
    "                 'sea_lvl_press_hectoPa', 'wnd_dir_360', 'wnd_spd_mtrpersec',\n",
    "                 'sky_condition', 'precip_hrly', 'precip_6hr_accum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the python libraries that I use.\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the table defining the available data. \n",
    "# There is a row for each station and it includes the begin and end date of available data.\n",
    "isd_stations_data = pd.read_csv('ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-history.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want data for DC, so I've chosen to search for the local airport. Reagan National Airport (DCA).\n",
    "# Note that all of the Station Names are uppercase.\n",
    "DCA_search = isd_stations_data.loc[(isd_stations_data['STATION NAME'].isna() == False) \n",
    "                                   & (isd_stations_data['STATION NAME'].str.contains('REAGAN'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing out the BEGIN and END years to create the range of years for which I'll download data.\n",
    "start_year = str(list(DCA_search.BEGIN)[0])[0:4]\n",
    "end_year = str(list(DCA_search.END)[0])[0:4]\n",
    "year_range = range(int(start_year), int(end_year)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the station ID by which the ftp site is organized.\n",
    "# Note that it is the concatenation of two columns separated by a hyphen.\n",
    "station_id = str(list(DCA_search.USAF)[0])+'-'+str(list(DCA_search.WBAN)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to loop through a given station ID for a given range of years.\n",
    "def download_isd_lite(station_id, year_range):\n",
    "    isd_df = pd.DataFrame()\n",
    "    for year in year_range:\n",
    "        # There can be gaps of missing years in the data, so try and except were required. \n",
    "        # The gaps that I've seen are only from decades ago.\n",
    "        try:\n",
    "            new_isd_df = pd.read_fwf('ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-lite/'\n",
    "                                     +str(year)+'/'\n",
    "                                     +station_id+'-'\n",
    "                                     +str(year)\n",
    "                                     +'.gz',\n",
    "                                     header=None)\n",
    "            isd_df = pd.concat([isd_df, new_isd_df])\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Resetting the index of the concatenated DataFrame\n",
    "    isd_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # Setting the column names that I've derived from the format guide\n",
    "    isd_df.columns = isd_fwf_cols\n",
    "   \n",
    "    # NOAA populates missing values with -9999, but I've chosen to replace them with NaN's.\n",
    "    isd_df.replace({-9999: np.nan}, inplace=True)\n",
    "    \n",
    "    # Some of the columns are scaled by a factor of 10 to eliminate decimal points,\n",
    "    # which would complicate the fixed width format that NOAA has chosen to utilize\n",
    "    scaled_columns = ['air_temp_c', 'dew_pt_temp_c', 'sea_lvl_press_hectoPa', \n",
    "                  'wnd_spd_mtrpersec', 'precip_hrly', 'precip_6hr_accum']\n",
    "    scaling_factor = 10\n",
    "    # Resolving the scaling factor\n",
    "    isd_df[scaled_columns] = isd_df[scaled_columns] / 10\n",
    "    \n",
    "    # Creating a date_time column from the various time-based columns NOAA provides.\n",
    "    # The first step is creating a properly formatted string that pandas can parse, and then parse them.\n",
    "    isd_df['date_time'] = isd_df.day.astype('int').astype('str').str.zfill(2)+'/'\\\n",
    "                         +isd_df.month.astype('int').astype('str').str.zfill(2)+'/'\\\n",
    "                         +isd_df.year.astype('int').astype('str')+'/'\\\n",
    "                         +isd_df.hour.astype('int').astype('str').str.zfill(2)\n",
    "    isd_df['date_time'] = pd.to_datetime(isd_df['date_time'], format='%d/%m/%Y/%H')\n",
    "    \n",
    "    return isd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the function for DCA for all years. This data frame can then be manipulated or exported.\n",
    "isd_df = download_isd_lite(station_id, year_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excel Users\n",
    "You could create the ftp URLs in Excel and then manually click all the links you create. If you had a column with the station_id and a column with the year for each file you want then you could construct the URLs as follows:\n",
    "\n",
    "=HYPERLINK(\"ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-lite/\"&reference_year_cell&\"/\"&reference_station_id_cell&\"-\"&reference_year_cell&\".gz\")\n",
    "\n",
    "You can then unzip each file, and open it in Excel. Using Excel's \"Text to Columns\" feature with the \"Original data type\" option set to \"Fixed width\", Excel will correctly separate the data in to columns. You can then manually add the column headers and save the data as an Excel file as desired. Then you could manually aggregate the data for multiple years and/or stations as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping the 1000 Active US Airport Weather Stations with the Longest Recorded History\n",
    "\n",
    "The map created below shows active (defined as having reported weather data in the last 2 days) weather stations at US airports. To ensure the map could be rendered, only 1000 weather stations were selected and those 1000 were selected based on having the oldest beginning date of data availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing weather stations without valid coordinates\n",
    "stations_with_coordinates = isd_stations_data.loc[((isd_stations_data.LAT != 0) \n",
    "                                                  & (isd_stations_data.LON != 0))\n",
    "                                                  & (isd_stations_data.LAT.isna() == False)\n",
    "                                                  & (isd_stations_data.LON.isna() == False)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of the latitude and longitude coordinates, which will be fed into the mapping function\n",
    "stations_with_coordinates['COORDINATES'] = list(zip(round(stations_with_coordinates.LAT,3), \n",
    "                                                    round(stations_with_coordinates.LON,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to reduce the volume so a reasonable map can be rendered, and only active sites are included\n",
    "most_recent_day = stations_with_coordinates['END'].max()\n",
    "us_airports_with_coordinates = stations_with_coordinates.loc[(stations_with_coordinates.CTRY == 'US')\n",
    "                                                             & (stations_with_coordinates.ICAO.isna() == False)\n",
    "                                                             & (stations_with_coordinates.END > (most_recent_day-2))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a string with the plain language Station Name and the Station ID used in the ftp site, \n",
    "# which will be presented on the map\n",
    "us_airports_with_coordinates['IDENTIFIER'] = (us_airports_with_coordinates['STATION NAME']+'; Station ID: '\n",
    "                                              +us_airports_with_coordinates['USAF'].astype('str')+'-'\n",
    "                                              +us_airports_with_coordinates['WBAN'].astype('str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting data so that when I trim the number of entries using .head(), I preserve those with the longest history of data\n",
    "us_airports_with_coordinates.sort_values(by=['BEGIN'], ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a map of the selected airport weather stations with annotations\n",
    "base_map = folium.Map([52, -112], zoom_start=3)\n",
    "# Limiting the map to 1000 locations\n",
    "for index, coord in enumerate(us_airports_with_coordinates['COORDINATES'].head(1000)):\n",
    "    marker = folium.Marker([coord[0],coord[1]], popup=us_airports_with_coordinates.iloc[index]['IDENTIFIER']).add_to(base_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the interactive map that you can use to find the Station ID for a given US location.\n",
    "# I've commented it out because because it causes rendering issues in Medium\n",
    "# base_map.save('1000_US_Airports_Weather_Stations_Map.html')\n",
    "# base_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
